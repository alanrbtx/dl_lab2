{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    get_peft_model, \n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    "    )\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"huggyllama/Llama-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, \n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map={\":\", 0})\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"AlanRobotics/saiga\")\n",
    "dataset = dataset.map(lambda example: tokenizer(example[\"instructions\"], example[\"outputs\"]), batched=True)\n",
    "dataset = dataset[\"train\"].train_test_split(0.1, 0.9)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"llama\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=2,\n",
    "    logging_steps=2000,\n",
    "    save_steps=2000,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"mistral-peft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, GPT2Tokenizer, AdamW, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# checkpoint = \"huggyllama/Llama-7b\"\n",
    "checkpoint = \"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(model, 'mistral-peft')\n",
    "print(model.get_memory_footprint())\n",
    "\n",
    "#model.push_to_hub('AlanRobotics/llama-chat', token='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')\n",
    "\n",
    "def create_query(prompt):\n",
    "    tokenized_sentence = tokenizer(prompt, return_tensors='pt')\n",
    "    res = model.generate(**tokenized_sentence, max_new_tokens=50, eos_token_id=13)\n",
    "    print(tokenizer.decode(res[0], skip_special_tokens=True))\n",
    "\n",
    "\n",
    "sentence = \"\"\"User: Что такое ИИ?\n",
    "Clone:\"\"\"\n",
    "create_query(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
